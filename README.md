# ffn_via_attention

Implements the components of a transformer (including feedforward networks) entirely via attention heads

See associated blog post/writeup at https://aizi.substack.com
